{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIZ5074inSdOAU/CFHloiO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/endalkK/gud.ai/blob/main/gud.ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch\n",
        "https://en.wikipedia.org/wiki/Torch_(machine_learning)\n",
        "\n",
        "Softwmax function\n",
        "https://en.wikipedia.org/wiki/Softmax_function\n",
        "\n",
        "Cross_entropy\n",
        "https://en.wikipedia.org/wiki/Cross-entropy\n",
        "\n",
        "Torch randint\n",
        "https://pytorch.org/docs/stable/generated/torch.randint.html\n",
        "\n",
        "https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
        "\n",
        "https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=yhdOAd6-wXkZ\n",
        "\n",
        "https://github.com/geezorg/ebooks/blob/master/amharic/literature/Oromay-BaaluGirma/src/Oromay-BaaluGirma.utf8.txt"
      ],
      "metadata": {
        "id": "9OfC9D4P8ZBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMrMqXtAK9iO",
        "outputId": "5f87e0ad-aedb-46f8-a79e-789a470d5202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txSRveFVI6Wl",
        "outputId": "e6706811-a5c7-4e67-b75c-749b8f2dc8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 16:19:02--  https://raw.githubusercontent.com/endalkK/gud.ai/refs/heads/main/Oromay-BaaluGirma.utf8.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 943366 (921K) [text/plain]\n",
            "Saving to: â€˜Oromay-BaaluGirma.utf8.txt.4â€™\n",
            "\n",
            "\r          Oromay-Ba   0%[                    ]       0  --.-KB/s               \rOromay-BaaluGirma.u 100%[===================>] 921.26K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-10 16:19:02 (24.9 MB/s) - â€˜Oromay-BaaluGirma.utf8.txt.4â€™ saved [943366/943366]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/endalkK/gud.ai/refs/heads/main/Oromay-BaaluGirma.utf8.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Oromay-BaaluGirma.utf8.txt.1', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "Jjh9B2g3J7DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjqy-qIJ_VZ",
        "outputId": "efc399a1-5f9f-49e5-9b91-11c2b4ed8424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  389076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nor_6L29KCxr",
        "outputId": "46c22e2e-3e04-4d0c-ee9f-08dd09f09601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "áŒˆáŒ½ áŠ áŠ•á‹µ \n",
            "\n",
            "   \n",
            "     á‹­áˆ… á‹¨áŒ¸áŒ‹á‹¬ áŠƒá‹­áˆˆ áˆ›áˆ­á‹«áˆ á‰áŠ•áŒ½áˆ á‰³áˆªáŠ­ áŠá‹ á¢ áŒ¸áŒ‹á‹¬ áŠƒá‹­áˆˆ áˆ›áˆ­á‹«áˆ áˆ›áŠá‹ ? áŠ á‰µá‰¸áŠ©áˆ á¢ á‰ á‹šáˆ… á‹“áˆˆáˆ áˆáŠ• á‹¨áˆšá‹«áˆµá‰¸áŠ©áˆ áŠáŒˆáˆ­ áŠ áˆˆ ? á‰£á‹­á‰¸áŠ©áˆ‰áˆ áˆ•á‹­á‹ˆá‰µ áˆ«áˆ· áŠ áŒ­áˆ­ áŠ“á‰µ á¢ á‰³áˆªáŠ©áŠ• áˆ«áˆµáˆ…\n",
            "á‰µá‹°áˆ­áˆµá‰ á‰³áˆˆáˆ…  á¢ á‰ áˆ«áˆ± á‰‹áŠ•á‰‹áŠ“ áŠ áˆµá‰°áˆ³áˆ°á‰¥  á¤ áˆµáˆœá‰µáŠ“ áŠ¥á‹­á‰³ á¤ á‹µáˆ­áŒŠá‰µáŠ“ á‹áˆá‰³ áŒ¸áŒ‹á‹¬ áˆ«áˆ± á‰³áˆª\n",
            "áŠ©áŠ• á‹­áŒˆáˆáŒ½áˆáˆ€áˆ á¢\n",
            "\n",
            "     áŠ¥áˆ½ áŠ¥áˆ±áˆµ á‹­áˆáŠ• á¤ á‹­áˆ…áŠ• á‹¨áˆáŠáŒáˆ«á‰½áˆ áŠ¥áŠ” áˆ›áŠáŠ ? áŒá‹µ á‹¨áˆˆáˆ á¢ á‹¨á‰°áˆˆá‹¨ áˆµáˆáŠ“ áˆ˜áˆáŠ­ á¤ áŒŠá‹œáŠ“ áˆ¥ááˆ« á‹¨áˆˆáŠáˆ á¢ áŠ¥áŠ” áŠáŠ áˆ›áˆˆá‰µ á‹­á‰ á‰ƒáˆ á¢ áŠá‰ áˆ­áŠ© á¤ áŠ áˆˆáˆ á¤ áŠ¥áŠ–áˆ«áˆˆáˆ á¢ áŒ¸áŒ‹á‹¬ á‹áˆ±áŠ• á‰°áˆ« áˆŸá‰½ áˆµáˆˆáˆ†áŠ á‰ áˆáˆ‰áˆ áŒŠá‹œáŠ“ á‰¦á‰³ á‹¨áˆ˜áŒˆáŠ˜á‰µ á‰½áˆá‰³ á‹¨áˆˆá‹áˆ á¢ áˆµáˆˆá‹šáˆ… áŠ¥áˆ± á‰ áˆ›á‹­áŒˆáŠá‰ á‰µ áŒŠá‹œáŠ“ áˆ¥ááˆ« áŠ áŠ•á‹³áŠ•á‹´ á‰¥á‰… áŠ¥áˆ‹áˆˆáˆ á¢ áˆ›áŠ• á‹­áˆ†áŠ• ? á‰¥áˆˆáˆ… áŠ á‰µáˆ˜áˆ«áˆ˜áˆ­ á¢ áŒ á‰ƒáˆš áŠ á‹­á‹°áˆˆáˆ á¢  áŒáŠ•  á‹¨áˆ°á‹  áŒ­áŠ•\n",
            "á‰€á‰µ áˆ›á‹¨á‰µ áŠ áˆá‹ˆá‹µáˆáŠ“ áŠ¨áˆá‰µáŒ¨áŠá‰… áˆáŠ•áŒˆáˆ­áˆ… á¢ á‹°áˆ«áˆ²á‹ áŠáŠ á¢\n",
            "\n",
            "     áŒ¸áŒ‹á‹¬áŠ• áŠ¨áˆ«áˆ± á‹­á‰ áˆáŒ¥ áŠ á‹á‰€á‹‹áˆˆáˆ á¢ á‹¨áŠ¥áŒá‹œáˆ­ á‰ áŒ áŠá‹ á¢ á‰ áˆ•á‹­á‹ˆá‰µ á‹¨áˆšá‹«á‹¨á‹áŠ• áŠáŒˆáˆ­ áˆáˆ‰ á‹­á‹ˆá‹³áˆ á¢ á‹á‹´á‰³á‹ á‰¥á‹™á‹áŠ• áŒŠá‹œ áŠ¨áŠ á‰…áˆ™ á‰ áˆ‹á‹­ á‹­áˆ†áŠ•á‰ á‰³áˆ á¢ á‰ á‹“áˆˆáˆ áˆ‹á‹­ áˆˆáˆšá‹«á‹¨á‹ á‹á‰ á‰µ áˆáˆ‰ áˆá‰¡ á‰ á‰‚ á‰¦á‰³ áˆµáˆˆáˆŒáˆˆá‹ á‹¨áˆšáŒ¨áŠá‰… áŒáŠ• á‰°áˆ« áˆŸá‰½ á‹¨áˆ†áŠ ááŒ¡áˆ­ áŠá‹ á¢ áŠ¥áŠ•á‹²á‹«á‹áˆ á‰ á‰…áˆ­á‰¥ áŠ«á‹ˆá‰á‰µáˆáŠ áŠá‹  á¢ áŒáŠ• áˆ›áŠá‹  áˆáŠ  á‹«áˆáˆ†áŠ !  áŠ¥áŠ•á‹³áˆáŠ©á‰µ  á‹«á‹¨á‹áŠ• áŠáŒˆáˆ­ áˆáˆ‰ á‹­á‹ˆá‹³áˆ á¤ áŠ¥áŠ“ áˆáˆ‰áŠ•áˆ á‹«\n",
            "áŒ£áˆ á¢\n",
            "\n",
            "     áŒ¸áŒ‹á‹¬ áˆ«áˆ± á‰¢áŠ“áŒˆáˆ­ á‹­áˆ»áˆ‹áˆ á¢  á‹¨á‰µ á‰¢áŒ€áˆáˆ¨á‹ á‹­áˆ»áˆ á‹­áˆ†áŠ• ? á‰½áŒáˆ© áˆáŠ•á‹µáŠá‹ ? á‹¨áˆ•á‹­á‹ˆá‰µ áˆ‚á‹°á‰µ á‹¨áˆšáŒ€áˆ˜áˆ­á‰ á‰µáŠ•áŠ“ á‹¨áˆšáˆáŒ¸áˆá‰ á‰µáŠ•  á‰ á‹áˆ á‹¨áˆšá‹«á‹á‰… áˆ›áŠ• áŠ áˆˆ ? á‰áŠ•áŒ½áˆ á‰³áˆªáŠ©áŠ• á‹¨á‰µáˆ áˆŠáŒ€áˆáˆ¨á‹ á‹­á‰½áˆ‹áˆ á¢ áŒ¸áŒ‹á‹¬ á‹­áŠ¸á‹áˆ‹á‰½áˆ á¡-\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "áŠ¦áˆ®áˆ›á‹­ \n",
            "\n",
            "\n",
            "                           áˆ›áˆˆá‹³ áˆ›áˆˆá‹³ \n",
            "\n",
            "                                áŒ€áŒ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVk8gVtDLJA_",
        "outputId": "7c367c3a-f7eb-43f9-b373-ae35cc529145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !'()-./0123456789?EJS^acdeiklmnorstuÂ«Â»áˆ€áˆáˆ‚áˆƒáˆ„áˆ…áˆ†áˆˆáˆ‰áˆŠáˆ‹áˆŒáˆáˆáˆáˆáˆ‘áˆ’áˆ”áˆ•áˆ˜áˆ™áˆšáˆ›áˆœáˆáˆáˆŸáˆ áˆ¡áˆ£áˆ¤áˆ¥áˆ¦áˆ¨áˆ©áˆªáˆ«áˆ¬áˆ­áˆ®áˆ¯áˆ°áˆ±áˆ²áˆ³áˆ´áˆµáˆ¶áˆ·áˆ¸áˆ¹áˆºáˆ»áˆ¼áˆ½áˆ¾áˆ¿á‰€á‰á‰‚á‰ƒá‰„á‰…á‰†á‰‹á‰á‰‘á‰“á‰•á‰ á‰¡á‰¢á‰£á‰¤á‰¥á‰¦á‰§á‰¨á‰ªá‰«á‰¬á‰­á‰®á‰°á‰±á‰²á‰³á‰´á‰µá‰¶á‰·á‰¸á‰¹á‰ºá‰»á‰¼á‰½á‰¾á‰¿áŠƒáŠ…áŠ‹áŠáŠ‘áŠ’áŠ“áŠ”áŠ•áŠ–áŠ—áŠ˜áŠ™áŠ›áŠœáŠáŠáŠ áŠ¡áŠ¢áŠ¤áŠ¥áŠ¦áŠ§áŠ¨áŠ©áŠªáŠ«áŠ¬áŠ­áŠ®áŠ°áŠ³áŠ¸áŠ¹áŠºáŠ»áŠ½á‹ˆá‹Šá‹‹á‹Œá‹á‹á‹á‹‘á‹“á‹”á‹•á‹˜á‹™á‹šá‹›á‹œá‹á‹á‹Ÿá‹ á‹¡á‹¢á‹£á‹¤á‹¥á‹¦á‹¨á‹©á‹«á‹¬á‹­á‹®á‹°á‹±á‹²á‹³á‹´á‹µá‹¶á‹·áŒ€áŒáŒ‚áŒƒáŒ„áŒ…áŒ†áŒ‡áŒˆáŒ‰áŒŠáŒ‹áŒŒáŒáŒáŒáŒ“áŒ•áŒ áŒ¡áŒ¢áŒ£áŒ¤áŒ¥áŒ¦áŒ§áŒ¨áŒ©áŒªáŒ«áŒ­áŒ®áŒ¯áŒ²áŒ³áŒ´áŒµáŒ¸áŒ¹áŒ»áŒ¼áŒ½áŒ¾áŒ¿á€áá‚áƒá„á…áˆá‰áŠá‹áŒááááá‘á’á“á”á•á–á¡á¢á¤\n",
            "287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "print(\"stio: \", stoi)\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "print(\"itos: \", itos)\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# print(encode(\"hii there\"))\n",
        "# print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZX6Mq8pLMUL",
        "outputId": "9b9a0eab-936d-407d-e499-4bb3d2e453f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stio:  {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, \"'\": 4, '(': 5, ')': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, '?': 20, 'E': 21, 'J': 22, 'S': 23, '^': 24, 'a': 25, 'c': 26, 'd': 27, 'e': 28, 'i': 29, 'k': 30, 'l': 31, 'm': 32, 'n': 33, 'o': 34, 'r': 35, 's': 36, 't': 37, 'u': 38, 'Â«': 39, 'Â»': 40, 'áˆ€': 41, 'áˆ': 42, 'áˆ‚': 43, 'áˆƒ': 44, 'áˆ„': 45, 'áˆ…': 46, 'áˆ†': 47, 'áˆˆ': 48, 'áˆ‰': 49, 'áˆŠ': 50, 'áˆ‹': 51, 'áˆŒ': 52, 'áˆ': 53, 'áˆ': 54, 'áˆ': 55, 'áˆ': 56, 'áˆ‘': 57, 'áˆ’': 58, 'áˆ”': 59, 'áˆ•': 60, 'áˆ˜': 61, 'áˆ™': 62, 'áˆš': 63, 'áˆ›': 64, 'áˆœ': 65, 'áˆ': 66, 'áˆ': 67, 'áˆŸ': 68, 'áˆ ': 69, 'áˆ¡': 70, 'áˆ£': 71, 'áˆ¤': 72, 'áˆ¥': 73, 'áˆ¦': 74, 'áˆ¨': 75, 'áˆ©': 76, 'áˆª': 77, 'áˆ«': 78, 'áˆ¬': 79, 'áˆ­': 80, 'áˆ®': 81, 'áˆ¯': 82, 'áˆ°': 83, 'áˆ±': 84, 'áˆ²': 85, 'áˆ³': 86, 'áˆ´': 87, 'áˆµ': 88, 'áˆ¶': 89, 'áˆ·': 90, 'áˆ¸': 91, 'áˆ¹': 92, 'áˆº': 93, 'áˆ»': 94, 'áˆ¼': 95, 'áˆ½': 96, 'áˆ¾': 97, 'áˆ¿': 98, 'á‰€': 99, 'á‰': 100, 'á‰‚': 101, 'á‰ƒ': 102, 'á‰„': 103, 'á‰…': 104, 'á‰†': 105, 'á‰‹': 106, 'á‰': 107, 'á‰‘': 108, 'á‰“': 109, 'á‰•': 110, 'á‰ ': 111, 'á‰¡': 112, 'á‰¢': 113, 'á‰£': 114, 'á‰¤': 115, 'á‰¥': 116, 'á‰¦': 117, 'á‰§': 118, 'á‰¨': 119, 'á‰ª': 120, 'á‰«': 121, 'á‰¬': 122, 'á‰­': 123, 'á‰®': 124, 'á‰°': 125, 'á‰±': 126, 'á‰²': 127, 'á‰³': 128, 'á‰´': 129, 'á‰µ': 130, 'á‰¶': 131, 'á‰·': 132, 'á‰¸': 133, 'á‰¹': 134, 'á‰º': 135, 'á‰»': 136, 'á‰¼': 137, 'á‰½': 138, 'á‰¾': 139, 'á‰¿': 140, 'áŠƒ': 141, 'áŠ…': 142, 'áŠ‹': 143, 'áŠ': 144, 'áŠ‘': 145, 'áŠ’': 146, 'áŠ“': 147, 'áŠ”': 148, 'áŠ•': 149, 'áŠ–': 150, 'áŠ—': 151, 'áŠ˜': 152, 'áŠ™': 153, 'áŠ›': 154, 'áŠœ': 155, 'áŠ': 156, 'áŠ': 157, 'áŠ ': 158, 'áŠ¡': 159, 'áŠ¢': 160, 'áŠ¤': 161, 'áŠ¥': 162, 'áŠ¦': 163, 'áŠ§': 164, 'áŠ¨': 165, 'áŠ©': 166, 'áŠª': 167, 'áŠ«': 168, 'áŠ¬': 169, 'áŠ­': 170, 'áŠ®': 171, 'áŠ°': 172, 'áŠ³': 173, 'áŠ¸': 174, 'áŠ¹': 175, 'áŠº': 176, 'áŠ»': 177, 'áŠ½': 178, 'á‹ˆ': 179, 'á‹Š': 180, 'á‹‹': 181, 'á‹Œ': 182, 'á‹': 183, 'á‹': 184, 'á‹': 185, 'á‹‘': 186, 'á‹“': 187, 'á‹”': 188, 'á‹•': 189, 'á‹˜': 190, 'á‹™': 191, 'á‹š': 192, 'á‹›': 193, 'á‹œ': 194, 'á‹': 195, 'á‹': 196, 'á‹Ÿ': 197, 'á‹ ': 198, 'á‹¡': 199, 'á‹¢': 200, 'á‹£': 201, 'á‹¤': 202, 'á‹¥': 203, 'á‹¦': 204, 'á‹¨': 205, 'á‹©': 206, 'á‹«': 207, 'á‹¬': 208, 'á‹­': 209, 'á‹®': 210, 'á‹°': 211, 'á‹±': 212, 'á‹²': 213, 'á‹³': 214, 'á‹´': 215, 'á‹µ': 216, 'á‹¶': 217, 'á‹·': 218, 'áŒ€': 219, 'áŒ': 220, 'áŒ‚': 221, 'áŒƒ': 222, 'áŒ„': 223, 'áŒ…': 224, 'áŒ†': 225, 'áŒ‡': 226, 'áŒˆ': 227, 'áŒ‰': 228, 'áŒŠ': 229, 'áŒ‹': 230, 'áŒŒ': 231, 'áŒ': 232, 'áŒ': 233, 'áŒ': 234, 'áŒ“': 235, 'áŒ•': 236, 'áŒ ': 237, 'áŒ¡': 238, 'áŒ¢': 239, 'áŒ£': 240, 'áŒ¤': 241, 'áŒ¥': 242, 'áŒ¦': 243, 'áŒ§': 244, 'áŒ¨': 245, 'áŒ©': 246, 'áŒª': 247, 'áŒ«': 248, 'áŒ­': 249, 'áŒ®': 250, 'áŒ¯': 251, 'áŒ²': 252, 'áŒ³': 253, 'áŒ´': 254, 'áŒµ': 255, 'áŒ¸': 256, 'áŒ¹': 257, 'áŒ»': 258, 'áŒ¼': 259, 'áŒ½': 260, 'áŒ¾': 261, 'áŒ¿': 262, 'á€': 263, 'á': 264, 'á‚': 265, 'áƒ': 266, 'á„': 267, 'á…': 268, 'áˆ': 269, 'á‰': 270, 'áŠ': 271, 'á‹': 272, 'áŒ': 273, 'á': 274, 'á': 275, 'á': 276, 'á': 277, 'á‘': 278, 'á’': 279, 'á“': 280, 'á”': 281, 'á•': 282, 'á–': 283, 'á¡': 284, 'á¢': 285, 'á¤': 286}\n",
            "itos:  {0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: \"'\", 5: '(', 6: ')', 7: '-', 8: '.', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: '?', 21: 'E', 22: 'J', 23: 'S', 24: '^', 25: 'a', 26: 'c', 27: 'd', 28: 'e', 29: 'i', 30: 'k', 31: 'l', 32: 'm', 33: 'n', 34: 'o', 35: 'r', 36: 's', 37: 't', 38: 'u', 39: 'Â«', 40: 'Â»', 41: 'áˆ€', 42: 'áˆ', 43: 'áˆ‚', 44: 'áˆƒ', 45: 'áˆ„', 46: 'áˆ…', 47: 'áˆ†', 48: 'áˆˆ', 49: 'áˆ‰', 50: 'áˆŠ', 51: 'áˆ‹', 52: 'áˆŒ', 53: 'áˆ', 54: 'áˆ', 55: 'áˆ', 56: 'áˆ', 57: 'áˆ‘', 58: 'áˆ’', 59: 'áˆ”', 60: 'áˆ•', 61: 'áˆ˜', 62: 'áˆ™', 63: 'áˆš', 64: 'áˆ›', 65: 'áˆœ', 66: 'áˆ', 67: 'áˆ', 68: 'áˆŸ', 69: 'áˆ ', 70: 'áˆ¡', 71: 'áˆ£', 72: 'áˆ¤', 73: 'áˆ¥', 74: 'áˆ¦', 75: 'áˆ¨', 76: 'áˆ©', 77: 'áˆª', 78: 'áˆ«', 79: 'áˆ¬', 80: 'áˆ­', 81: 'áˆ®', 82: 'áˆ¯', 83: 'áˆ°', 84: 'áˆ±', 85: 'áˆ²', 86: 'áˆ³', 87: 'áˆ´', 88: 'áˆµ', 89: 'áˆ¶', 90: 'áˆ·', 91: 'áˆ¸', 92: 'áˆ¹', 93: 'áˆº', 94: 'áˆ»', 95: 'áˆ¼', 96: 'áˆ½', 97: 'áˆ¾', 98: 'áˆ¿', 99: 'á‰€', 100: 'á‰', 101: 'á‰‚', 102: 'á‰ƒ', 103: 'á‰„', 104: 'á‰…', 105: 'á‰†', 106: 'á‰‹', 107: 'á‰', 108: 'á‰‘', 109: 'á‰“', 110: 'á‰•', 111: 'á‰ ', 112: 'á‰¡', 113: 'á‰¢', 114: 'á‰£', 115: 'á‰¤', 116: 'á‰¥', 117: 'á‰¦', 118: 'á‰§', 119: 'á‰¨', 120: 'á‰ª', 121: 'á‰«', 122: 'á‰¬', 123: 'á‰­', 124: 'á‰®', 125: 'á‰°', 126: 'á‰±', 127: 'á‰²', 128: 'á‰³', 129: 'á‰´', 130: 'á‰µ', 131: 'á‰¶', 132: 'á‰·', 133: 'á‰¸', 134: 'á‰¹', 135: 'á‰º', 136: 'á‰»', 137: 'á‰¼', 138: 'á‰½', 139: 'á‰¾', 140: 'á‰¿', 141: 'áŠƒ', 142: 'áŠ…', 143: 'áŠ‹', 144: 'áŠ', 145: 'áŠ‘', 146: 'áŠ’', 147: 'áŠ“', 148: 'áŠ”', 149: 'áŠ•', 150: 'áŠ–', 151: 'áŠ—', 152: 'áŠ˜', 153: 'áŠ™', 154: 'áŠ›', 155: 'áŠœ', 156: 'áŠ', 157: 'áŠ', 158: 'áŠ ', 159: 'áŠ¡', 160: 'áŠ¢', 161: 'áŠ¤', 162: 'áŠ¥', 163: 'áŠ¦', 164: 'áŠ§', 165: 'áŠ¨', 166: 'áŠ©', 167: 'áŠª', 168: 'áŠ«', 169: 'áŠ¬', 170: 'áŠ­', 171: 'áŠ®', 172: 'áŠ°', 173: 'áŠ³', 174: 'áŠ¸', 175: 'áŠ¹', 176: 'áŠº', 177: 'áŠ»', 178: 'áŠ½', 179: 'á‹ˆ', 180: 'á‹Š', 181: 'á‹‹', 182: 'á‹Œ', 183: 'á‹', 184: 'á‹', 185: 'á‹', 186: 'á‹‘', 187: 'á‹“', 188: 'á‹”', 189: 'á‹•', 190: 'á‹˜', 191: 'á‹™', 192: 'á‹š', 193: 'á‹›', 194: 'á‹œ', 195: 'á‹', 196: 'á‹', 197: 'á‹Ÿ', 198: 'á‹ ', 199: 'á‹¡', 200: 'á‹¢', 201: 'á‹£', 202: 'á‹¤', 203: 'á‹¥', 204: 'á‹¦', 205: 'á‹¨', 206: 'á‹©', 207: 'á‹«', 208: 'á‹¬', 209: 'á‹­', 210: 'á‹®', 211: 'á‹°', 212: 'á‹±', 213: 'á‹²', 214: 'á‹³', 215: 'á‹´', 216: 'á‹µ', 217: 'á‹¶', 218: 'á‹·', 219: 'áŒ€', 220: 'áŒ', 221: 'áŒ‚', 222: 'áŒƒ', 223: 'áŒ„', 224: 'áŒ…', 225: 'áŒ†', 226: 'áŒ‡', 227: 'áŒˆ', 228: 'áŒ‰', 229: 'áŒŠ', 230: 'áŒ‹', 231: 'áŒŒ', 232: 'áŒ', 233: 'áŒ', 234: 'áŒ', 235: 'áŒ“', 236: 'áŒ•', 237: 'áŒ ', 238: 'áŒ¡', 239: 'áŒ¢', 240: 'áŒ£', 241: 'áŒ¤', 242: 'áŒ¥', 243: 'áŒ¦', 244: 'áŒ§', 245: 'áŒ¨', 246: 'áŒ©', 247: 'áŒª', 248: 'áŒ«', 249: 'áŒ­', 250: 'áŒ®', 251: 'áŒ¯', 252: 'áŒ²', 253: 'áŒ³', 254: 'áŒ´', 255: 'áŒµ', 256: 'áŒ¸', 257: 'áŒ¹', 258: 'áŒ»', 259: 'áŒ¼', 260: 'áŒ½', 261: 'áŒ¾', 262: 'áŒ¿', 263: 'á€', 264: 'á', 265: 'á‚', 266: 'áƒ', 267: 'á„', 268: 'á…', 269: 'áˆ', 270: 'á‰', 271: 'áŠ', 272: 'á‹', 273: 'áŒ', 274: 'á', 275: 'á', 276: 'á', 277: 'á', 278: 'á‘', 279: 'á’', 280: 'á“', 281: 'á”', 282: 'á•', 283: 'á–', 284: 'á¡', 285: 'á¢', 286: 'á¤'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_wrLhZLtLL",
        "outputId": "0aab1e25-baac-4dbb-832f-5762f1076fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([389076]) torch.int64\n",
            "tensor([  1, 227, 260,   2, 158, 149, 216,   2,   1,   1,   2,   2,   2,   1,\n",
            "          2,   2,   2,   2,   2, 209,  46,   2, 205, 256, 230, 208,   2, 141,\n",
            "        209,  48,   2,  64,  80, 207,  66,   2, 100, 149, 260,  53,   2, 128,\n",
            "         77, 170,   2, 144, 183,   2, 285,   2, 256, 230, 208,   2, 141, 209,\n",
            "         48,   2,  64,  80, 207,  66,   2,  64, 144, 183,   2,  20,   2, 158,\n",
            "        130, 133, 166,  53,   2, 285,   2, 111, 192,  46,   2, 187,  48,  66,\n",
            "          2,  66, 149,   2, 205,  63, 207,  88, 133, 166,  53,   2, 144, 227,\n",
            "         80,   2, 158,  48,   2,  20,   2, 114, 209, 133, 166,  49,  66,   2,\n",
            "         60, 209, 179, 130,   2,  78,  90,   2, 158, 249,  80,   2, 147, 130,\n",
            "          2, 285,   2, 128,  77, 166, 149,   2,  78,  88,  46,   1, 130, 211,\n",
            "         80,  88, 111, 128,  48,  46,   2,   2, 285,   2, 111,  78,  84,   2,\n",
            "        106, 149, 106, 147,   2, 158,  88, 125,  86,  83, 116,   2,   2, 286,\n",
            "          2,  88,  65, 130, 147,   2, 162, 209, 128,   2, 286,   2, 216,  80,\n",
            "        229, 130, 147,   2, 195,  66, 128,   2, 256, 230, 208,   2,  78,  84,\n",
            "          2, 128,  77,   1, 166, 149,   2, 209, 227,  53, 260,  53,  41,  53,\n",
            "          2, 285,   1,   1,   2,   2,   2,   2,   2, 162,  96,   2, 162,  84,\n",
            "         88,   2, 209,  42, 149,   2, 286,   2, 209,  46, 149,   2, 205,  66,\n",
            "        144, 232,  78, 138,  42,   2, 162, 148,   2,  64, 144, 156,   2,  20,\n",
            "          2, 232, 216,   2, 205,  48,  66,   2, 285,   2, 205, 125,  48, 205,\n",
            "          2,  88,  66, 147,   2,  61,  53, 170,   2, 286,   2, 229, 194, 147,\n",
            "          2,  73, 274,  78,   2, 205,  48, 156,  66,   2, 285,   2, 162, 148,\n",
            "          2, 144, 156,   2,  64,  48, 130,   2, 209, 111, 102,  53,   2, 285,\n",
            "          2, 144, 111,  80, 166,   2, 286,   2, 158,  48,  42,   2, 286,   2,\n",
            "        162, 150,  78,  48,  42,   2, 285,   2, 256, 230, 208,   2, 183,  84,\n",
            "        149,   2, 125,  78,   2,  68, 138,   2,  88,  48,  47, 144,   2, 111,\n",
            "         42,  49,  66,   2, 229, 194, 147,   2, 117, 128,   2, 205,  61, 227,\n",
            "        152, 130,   2, 138,  54, 128,   2, 205,  48, 183,  66,   2, 285,   2,\n",
            "         88,  48, 192,  46,   2, 162,  84,   2, 111,  64, 209, 227, 156, 111,\n",
            "        130,   2, 229, 194, 147,   2,  73, 274,  78,   2, 158, 149, 214, 149,\n",
            "        215,   2, 116, 104,   2, 162,  51,  48,  42,   2, 285,   2,  64, 149,\n",
            "          2, 209,  47, 149,   2,  20,   2, 116,  48,  46,   2, 158, 130,  61,\n",
            "         78,  61,  80,   2, 285,   2, 237, 102,  63,   2, 158, 209, 211,  48,\n",
            "         66,   2, 285,   2,   2, 232, 149,   2,   2, 205,  83, 183,   2,   2,\n",
            "        249, 149,   1,  99, 130,   2,  64, 205, 130,   2, 158,  53, 179, 216,\n",
            "         66, 147,   2, 165,  66, 130, 245, 144, 104,   2,  53, 149, 227,  80,\n",
            "         46,   2, 285,   2, 211,  78,  85, 183,   2, 144, 156,   2, 285,   1,\n",
            "          1,   2,   2,   2,   2,   2, 256, 230, 208, 149,   2, 165,  78,  84,\n",
            "          2, 209, 111,  53, 242,   2, 158, 183,  99, 181,  48,  42,   2, 285,\n",
            "          2, 205, 162, 232, 194,  80,   2, 111, 232,   2, 144, 183,   2, 285,\n",
            "          2, 111,  60, 209, 179, 130,   2, 205,  63, 207, 205, 183, 149,   2,\n",
            "        144, 227,  80,   2,  42,  49,   2, 209, 179, 214,  53,   2, 285,   2,\n",
            "        183, 215, 128, 183,   2, 116, 191, 183, 149,   2, 229, 194,   2, 165,\n",
            "        158, 104,  62,   2, 111,  51, 209,   2, 209,  47, 149, 111, 128,  53,\n",
            "          2, 285,   2, 111, 187,  48,  66,   2,  51, 209,   2,  48,  63, 207,\n",
            "        205, 183,   2, 183, 111, 130,   2,  42,  49,   2,  53, 112,   2, 111,\n",
            "        101,   2, 117, 128,   2,  88,  48,  52,  48, 183,   2, 205,  63, 245,\n",
            "        144, 104,   2, 232, 149,   2, 125,  78,   2,  68, 138,   2, 205,  47,\n",
            "        144,   2, 274, 238,  80,   2, 144, 183,   2, 285,   2, 162, 149, 213,\n",
            "        207, 183,  66,   2, 111, 104,  80, 116,   2, 168, 179, 100, 130,  67,\n",
            "        156,   2, 144, 183,   2,   2, 285,   2, 232, 149,   2,  64, 144, 183,\n",
            "          2,   2,  67, 156,   2,   2, 207,  53,  47, 144,   2,   3,   2,   2,\n",
            "        162, 149, 214,  53, 166, 130,   2,   2, 207, 205, 183, 149,   2, 144,\n",
            "        227,  80,   2,  42,  49,   2, 209, 179, 214,  53,   2, 286,   2, 162,\n",
            "        147,   2,  42,  49, 149,  66,   2, 207,   1, 240,  53,   2, 285,   1,\n",
            "          1,   2,   2,   2,   2,   2, 256, 230, 208,   2,  78,  84,   2, 113,\n",
            "        147, 227,  80,   2, 209,  94,  51,  53,   2, 285,   2,   2, 205, 130,\n",
            "          2, 113, 219,  66,  75, 183,   2, 209,  94,  53,   2, 209,  47, 149,\n",
            "          2,  20,   2, 138, 232,  76,   2,  66, 149, 216, 144, 183,   2,  20,\n",
            "          2, 205,  60, 209, 179, 130,   2,  43, 211, 130,   2, 205,  63, 219,\n",
            "         61,  80, 111, 130, 149, 147,   2, 205,  63, 269, 256,  66, 111, 130,\n",
            "        149,   2,   2, 111, 183,  53,   2, 205,  63, 207, 183, 104,   2,  64,\n",
            "        149,   2, 158,  48,   2,  20,   2, 100, 149, 260,  53,   2, 128,  77,\n",
            "        166, 149,   2, 205, 130,  66,   2,  50, 219,  66,  75, 183,   2, 209,\n",
            "        138,  51,  53,   2, 285,   2, 256, 230, 208,   2, 209, 174, 183,  51,\n",
            "        138,  42,   2, 284,   7,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 163,  81,  64,\n",
            "        209,   2,   1,   1,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,  64,  48, 214,   2,  64,  48, 214,   2,   1,   1,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2, 219, 232])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "fVhGQ8A9NDn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUcXnGiUNLVY",
        "outputId": "e53e17e6-58ad-42d6-aa57-54bc2dd27a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1, 227, 260,   2, 158, 149, 216,   2,   1])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-OrvQwWNoTS",
        "outputId": "bc86ccf5-77e3-4150-89de-4ed5a0ee1032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([1]) the target: 227\n",
            "when input is tensor([  1, 227]) the target: 260\n",
            "when input is tensor([  1, 227, 260]) the target: 2\n",
            "when input is tensor([  1, 227, 260,   2]) the target: 158\n",
            "when input is tensor([  1, 227, 260,   2, 158]) the target: 149\n",
            "when input is tensor([  1, 227, 260,   2, 158, 149]) the target: 216\n",
            "when input is tensor([  1, 227, 260,   2, 158, 149, 216]) the target: 2\n",
            "when input is tensor([  1, 227, 260,   2, 158, 149, 216,   2]) the target: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6NDBww3QgJq",
        "outputId": "aa7d6d9c-3d82-44ea-ac02-01e8ca122ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350168, 38908)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "ix = torch.randint(1003854-8, (4,))\n",
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVWylRqDQwNq",
        "outputId": "aeb31a63-03e9-4b63-a6a9-37c48c908d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 76049, 234249, 934904, 560986])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "# y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "# x, y"
      ],
      "metadata": {
        "id": "pGDSOCCxSkn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F14cREjAN2Mk",
        "outputId": "ed920dd2-9694-41a9-8d6e-6c4c9c0a7c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[133,   2, 209,  46,   2, 116, 136,   2],\n",
            "        [ 61,  88,  51,  48, 138, 285,   2, 111],\n",
            "        [158,  48,   2,   1,  39,   2, 125, 170],\n",
            "        [130, 147,   2, 158, 166,  77,   2, 158]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  2, 209,  46,   2, 116, 136,   2, 158],\n",
            "        [ 88,  51,  48, 138, 285,   2, 111,  73],\n",
            "        [ 48,   2,   1,  39,   2, 125, 170,  51],\n",
            "        [147,   2, 158, 166,  77,   2, 158, 116]])\n",
            "----\n",
            "when input is [133] the target: 2\n",
            "when input is [133, 2] the target: 209\n",
            "when input is [133, 2, 209] the target: 46\n",
            "when input is [133, 2, 209, 46] the target: 2\n",
            "when input is [133, 2, 209, 46, 2] the target: 116\n",
            "when input is [133, 2, 209, 46, 2, 116] the target: 136\n",
            "when input is [133, 2, 209, 46, 2, 116, 136] the target: 2\n",
            "when input is [133, 2, 209, 46, 2, 116, 136, 2] the target: 158\n",
            "when input is [61] the target: 88\n",
            "when input is [61, 88] the target: 51\n",
            "when input is [61, 88, 51] the target: 48\n",
            "when input is [61, 88, 51, 48] the target: 138\n",
            "when input is [61, 88, 51, 48, 138] the target: 285\n",
            "when input is [61, 88, 51, 48, 138, 285] the target: 2\n",
            "when input is [61, 88, 51, 48, 138, 285, 2] the target: 111\n",
            "when input is [61, 88, 51, 48, 138, 285, 2, 111] the target: 73\n",
            "when input is [158] the target: 48\n",
            "when input is [158, 48] the target: 2\n",
            "when input is [158, 48, 2] the target: 1\n",
            "when input is [158, 48, 2, 1] the target: 39\n",
            "when input is [158, 48, 2, 1, 39] the target: 2\n",
            "when input is [158, 48, 2, 1, 39, 2] the target: 125\n",
            "when input is [158, 48, 2, 1, 39, 2, 125] the target: 170\n",
            "when input is [158, 48, 2, 1, 39, 2, 125, 170] the target: 51\n",
            "when input is [130] the target: 147\n",
            "when input is [130, 147] the target: 2\n",
            "when input is [130, 147, 2] the target: 158\n",
            "when input is [130, 147, 2, 158] the target: 166\n",
            "when input is [130, 147, 2, 158, 166] the target: 77\n",
            "when input is [130, 147, 2, 158, 166, 77] the target: 2\n",
            "when input is [130, 147, 2, 158, 166, 77, 2] the target: 158\n",
            "when input is [130, 147, 2, 158, 166, 77, 2, 158] the target: 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward method in the BigramLanguageModel class processes input sequences and returns:\n",
        "\n",
        "Logits â€“ The predicted next-token probabilities.\n",
        "Loss (if targets is provided) â€“ The training error.\n",
        "\n",
        "**B, T, C**\n",
        "\n",
        "*   B (Batch Size): The number of input sequences processed simultaneously.\n",
        "*   T (Sequence Length): The number of tokens in each sequence.\n",
        "*   C (Vocabulary Size): The number of possible tokens (i.e., vocab size).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Bar8u6V1qsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Softmax_function"
      ],
      "metadata": {
        "id": "dguddlEr8TZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "logits = torch.tensor([\n",
        "    [[0.1, 0.2, ..., 0.05], [0.3, 0.1, ..., 0.02], [0.05, 0.2, ..., 0.9]],  # Batch 1\n",
        "    [[0.4, 0.2, ..., 0.1], [0.2, 0.3, ..., 0.05], [0.1, 0.05, ..., 0.3]]   # Batch 2\n",
        "])  # Shape: (B=2, T=3, C=100)\n",
        "\n",
        "logits = torch.tensor([\n",
        "    [0.1, 0.2, ..., 0.05],  # Token 1 from Batch 1\n",
        "    [0.3, 0.1, ..., 0.02],  # Token 2 from Batch 1\n",
        "    [0.05, 0.2, ..., 0.9],  # Token 3 from Batch 1\n",
        "    [0.4, 0.2, ..., 0.1],   # Token 1 from Batch 2\n",
        "    [0.2, 0.3, ..., 0.05],  # Token 2 from Batch 2\n",
        "    [0.1, 0.05, ..., 0.3]   # Token 3 from Batch 2\n",
        "])  # Shape: (6, 100)\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hg0TjpMC5Wtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1: Discrete Case (Classification)\n",
        "Suppose we have a binary classification problem where an actual label (true distribution\n",
        "ğ‘\n",
        "p) is:\n",
        "\n",
        "ğ‘\n",
        "=\n",
        "[\n",
        "1\n",
        ",\n",
        "0\n",
        "]\n",
        "(i.e.,Â theÂ firstÂ classÂ isÂ theÂ trueÂ label)\n",
        "p=[1,0](i.e.,Â theÂ firstÂ classÂ isÂ theÂ trueÂ label)\n",
        "And a model predicts probabilities:\n",
        "\n",
        "ğ‘\n",
        "=\n",
        "[\n",
        "0.8\n",
        ",\n",
        "0.2\n",
        "]\n",
        "q=[0.8,0.2]\n",
        "The cross-entropy is:\n",
        "\n",
        "ğ»\n",
        "(\n",
        "ğ‘\n",
        ",\n",
        "ğ‘\n",
        ")\n",
        "=\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘¥\n",
        "ğ‘\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "log\n",
        "â¡\n",
        "ğ‘\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "H(p,q)=âˆ’\n",
        "x\n",
        "âˆ‘\n",
        "â€‹\n",
        " p(x)logq(x)\n",
        "Since the true distribution\n",
        "ğ‘\n",
        "p assigns full probability to the first class (1,0), we compute:\n",
        "\n",
        "ğ»\n",
        "(\n",
        "ğ‘\n",
        ",\n",
        "ğ‘\n",
        ")\n",
        "=\n",
        "âˆ’\n",
        "[\n",
        "1\n",
        "â‹…\n",
        "log\n",
        "â¡\n",
        "0.8\n",
        "+\n",
        "0\n",
        "â‹…\n",
        "log\n",
        "â¡\n",
        "0.2\n",
        "]\n",
        "H(p,q)=âˆ’[1â‹…log0.8+0â‹…log0.2]\n",
        "=\n",
        "âˆ’\n",
        "log\n",
        "â¡\n",
        "0.8\n",
        "=âˆ’log0.8\n",
        "=\n",
        "0.223\n",
        "(inÂ baseÂ 2,Â otherwiseÂ inÂ base\n",
        "ğ‘’\n",
        ",Â itÂ wouldÂ beÂ 0.096)\n",
        "=0.223(inÂ baseÂ 2,Â otherwiseÂ inÂ baseÂ e,Â itÂ wouldÂ beÂ 0.096)\n",
        "This means that if the model assigns high probability to the correct class, the cross-entropy is low."
      ],
      "metadata": {
        "id": "BBDVF1xJik11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits)\n",
        "print(logits.shape)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG8Yc-_5Zh9o",
        "outputId": "727eb11c-5d4b-4cc0-a849-31cd79467009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5928, -1.6397, -1.4249,  ..., -0.1259,  0.3837,  0.7042],\n",
            "        [ 0.7535, -0.5359, -1.0277,  ...,  0.6586, -1.3565, -1.8218],\n",
            "        [-0.2185, -0.4746,  0.8747,  ...,  1.0283, -1.0580, -2.5318],\n",
            "        ...,\n",
            "        [-0.9197, -0.7263,  1.3462,  ...,  0.5315,  1.0208,  0.8747],\n",
            "        [ 0.7535, -0.5359, -1.0277,  ...,  0.6586, -1.3565, -1.8218],\n",
            "        [-0.0565,  1.0043, -0.9937,  ..., -0.8984,  0.6702, -1.3248]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([32, 287])\n",
            "tensor(6.2408, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((1, 1), dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz8iD-pw7ju_",
        "outputId": "7efcad9c-180c-4834-ca09-1b5c60863588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "# logits, loss = m(torch.zeros((1, 1), dtype=torch.long))\n",
        "# print(logits[:, -1, :])\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckISyrgh8afh",
        "outputId": "ca041313-f286-4b1c-e2b3-d3ea10640592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 287])\n",
            "\tláŠ­9áˆ·áŠ½áˆ©áŠ¡áˆ£áˆ…á‰¤á‹´áˆ©áˆ¼náˆáˆ 88áŠ¡á…áˆšáŠ”áˆ‘áŒ³áˆ¦áŒ ^áˆ¤áˆ•áŒ¸áˆ¾á‹œsáˆ«á‹­áŒŒ.á‹´eáˆ¡á‹¡áŠáˆ¥á‹­áŠªáŠ”iáŠ˜á‹µ)á‹Ÿáˆšá‹¢á…áˆºáá‹Šáˆ†á‹á‹¬áá‹”áˆ¯á€á‹‹áŒ§áŒ²áŒ³-áŒ“á‹‘áˆªáŠ¨áŠ¬á‰§áŠ—áŒ¦á‰ cáŠ˜á‹¤á‹¬á…Â«áŠªá‹áŠ­áŠ‹áŠ‹áŠ˜áŒµáŒ£áˆ˜áŒ“áŒ¿á‰¦á€7Já\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-BYRY9nGKPH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Print loss every 10 epochs\n",
        "    if (steps + 1) % 10 == 0:\n",
        "        print(f\"Epoch {steps+1}/{100}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30SGNQ7IKMfK",
        "outputId": "77f47745-8c97-46e8-e350-cf170a1ab4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss: 6.2052\n",
            "Epoch 20/100, Loss: 6.3065\n",
            "Epoch 30/100, Loss: 6.2858\n",
            "Epoch 40/100, Loss: 6.2995\n",
            "Epoch 50/100, Loss: 6.2532\n",
            "Epoch 60/100, Loss: 6.2702\n",
            "Epoch 70/100, Loss: 6.1529\n",
            "Epoch 80/100, Loss: 6.1519\n",
            "Epoch 90/100, Loss: 6.1374\n",
            "Epoch 100/100, Loss: 6.1467\n",
            "6.146655559539795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMs8gh3YERwj",
        "outputId": "0cc9aa13-0abe-4593-bd45-d91ec183c9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t/3á‰¨áˆ‰á‹µ7á‚áŠ¸ áˆ†-áŒ¿á’á‰¢áˆµáˆšáŒ†áˆ áˆ‚á‹”á‹áŒ¼áŠºáˆ¡3a\tá‰¾áŒ¦áŠœáŒµsáŠ¸áˆ©áˆ¶á¢cáŒ‹0áˆ¡á‰ƒáŠ’.á‰“áŒ³áˆ áˆ˜áˆ¥áŒ¤áˆá’áˆ£áŠ°áˆ€á‹áˆ‰áŒ€áŠ›1á‹‹á‹¤áŠ®á‰¾áŒ¼áˆ³Já‰½áŒ€áˆ‘á‹¶ráˆ”áŠ½áŠ®ááˆ¸áŠáˆ²á‹ áˆ½áŒ§áŒµ'áˆ áŠ›á‰ºá‹á€eáŒ‹áŒµá‰«á‰“áˆáŒ“á‹á‰»á‰§á‹áŠ¥ká‹«áˆ·áˆ•áˆ´áˆ‚áŠ›áŠ°áˆ¨áŠ¨áˆ´áŒ­áˆ‹á‹·áŒµá‹áˆ¶áˆ„á‰á‹áˆ±áŠá…oá‹¢áŠ á‰‹á‰¢á‘á‰ á‹©áˆ¯áˆ‘áŠ§-á‰®!áŒáˆ™á‰¸á‰¸á‹°táŒ¾áá‰¾áˆ0á‰¥áˆ„áŒµá‹¨á‹¡rá‰¿n'áŠ‹á‹Šá‰á‰¹9á‹‘áˆ£á‰´ucáŒ„á‹™áŒá‰½á‰…áˆ©Eáˆ»á‰«á‰¨áˆŒáŒ»Â«áˆŒá‰µá‹Šá‹áŒ¦á‰·?áˆ˜Â»á‹ˆá‹”á‰®áˆ¤áŒµá‹¦áˆáŠ«á‰¦áŒ»á¡á‰·á•áŒ‰á‹Šá‰µáˆ¾áŒˆáŠœá‹¬áˆ¾á‹³á‹«á‰áŒ¦áŒ«á‰·áˆ£áŒƒáˆá‹3á‰§á‰áˆ›á‹™áŠ½á‰„á‰¶áŠ—32á‹¶á‹·á‰¿áŒŠá‹·áˆáŒ á‰¢áˆšá‰½áˆ9á‰¢)áŠ¤7áŠ½áŒ¡áˆ¨4á‰„cdáŒ»á‰“áŠ¦á‰§aáŠ¥áŒˆ.á‹á–á‰­áˆ¦Jáˆ”áŒ¨áŒ†áŠ«áŠ³áˆ‘^c9á‰¸áˆáŠ­áŒá‹Œáˆá‰€áŒ áŒ©áˆœá‰¼áŒ‚áˆ£á¤áŒ´áŒ‡Â»áˆ½áŠ®oáˆr8áŠ¦áƒáŠÂ«áˆ¬á‰ƒcáˆ´á‰•áˆ­áˆ•ááŒ¢áˆ€á‰\n",
            "áŠ…áŠ­áŠ•áŒŒáˆ¬áˆáŒƒá‹8áˆšáŠá‰‘á‰¨á‚áŠ¸áˆoáŠ‹3áŒƒ(3.áŠáŒµá‰·á‹¢áŠœeáˆ¿áˆ¹á‹Œá‰¤áŠ­áŒ®9áŒŠeáˆ¶láˆ€á‹‹áŒ§á‹á‰¶áŠ®^áŒµáŒˆ.á‰¦áŠ’áˆ¸á‰…á‰µá‰ºáˆ³á‰·áá‰»! á‰£á‹²áŠ¢áˆ†áˆƒá‰«á‹ŠáŠ½áˆ±á‹á”áˆˆáŒ¯áŒ‰áˆœáŠ‹7sáŠ‹áŠ©á‰¨á‘á‹¬dáˆá‹¦SáŠ—á‰¹á‰†á‹œáŠ°á‹«á‰áˆªá‰·náŒ­á‹‘á‹á‹‹!á“á‹¬Â«?áˆ¯ká…á‘á‹á‰‹áˆ®á‰»áŠ®á‹­á‹¢áŠƒáŠªáŠ”áŒ¹áŠ»áŒ¥áˆ„6á‹£áŒ¥Â»á‹láŒ¸á‘áŠ‘á‹³á‰á‰®á‹«ááŒ¸á‹áŒ©á‰³á‹¨áˆ«á‰‘á‹Ÿáˆ¬áˆ²cáŒ½3(á…áŒ‰á‰¿4á’á‹£áˆ‘9áˆ«á¡áˆ¬á‰»áˆá‹°áŠ©áŠ™(á‹£áŒmáŠ¤áŒ«0á‰´(áˆ¨á‹‹áŒá–á‹“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42 is arbitrary but widely used due to pop culture (from The Hitchhikerâ€™s Guide to the Galaxy, where 42 is \"the answer to life, the universe, and everything\")."
      ],
      "metadata": {
        "id": "rAgrcamrPbZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "\n",
        "b = torch.sum(a, 1, keepdim=True)\n",
        "B = torch.randint(0,10,(3,2)).float()\n",
        "a, b, a/b, B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPu297L-MPqW",
        "outputId": "88040c11-59c3-48fe-bc48-aa3dfda9d9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1., 0., 0.],\n",
              "         [1., 1., 0.],\n",
              "         [1., 1., 1.]]),\n",
              " tensor([[1.],\n",
              "         [2.],\n",
              "         [3.]]),\n",
              " tensor([[1.0000, 0.0000, 0.0000],\n",
              "         [0.5000, 0.5000, 0.0000],\n",
              "         [0.3333, 0.3333, 0.3333]]),\n",
              " tensor([[2., 7.],\n",
              "         [6., 4.],\n",
              "         [6., 5.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [[1.0],  # t=0\n",
        "     [2.0],  # t=1\n",
        "     [3.0],  # t=2\n",
        "     [4.0]],  # t=3\n",
        "    [[5.0],  # t=0\n",
        "     [6.0],  # t=1\n",
        "     [7.0],  # t=2\n",
        "     [8.0]],  # t=3\n",
        "])\n",
        "B, T, C = x.shape\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        print(xprev)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "        print(xbow[b,t])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkfUyEzmS9HC",
        "outputId": "4a18f63c-246f-4333-ca4f-c16b25bb0765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.]])\n",
            "tensor([1.])\n",
            "tensor([[1.],\n",
            "        [2.]])\n",
            "tensor([1.5000])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "tensor([2.])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.]])\n",
            "tensor([2.5000])\n",
            "tensor([[5.]])\n",
            "tensor([5.])\n",
            "tensor([[5.],\n",
            "        [6.]])\n",
            "tensor([5.5000])\n",
            "tensor([[5.],\n",
            "        [6.],\n",
            "        [7.]])\n",
            "tensor([6.])\n",
            "tensor([[5.],\n",
            "        [6.],\n",
            "        [7.],\n",
            "        [8.]])\n",
            "tensor([6.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFPZzJ2GQkcu",
        "outputId": "8b3342c1-caa0-4dec-a478-29230903f696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "print(xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyq0RLuDQsYS",
        "outputId": "39ce3664-9f7d-49d6-8c19-c0a400058742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0431, -1.6047],\n",
            "         [ 0.8724, -1.0414],\n",
            "         [ 0.5006, -1.0056],\n",
            "         [ 0.3134, -1.0563],\n",
            "         [ 0.0970, -0.6925],\n",
            "         [-0.1804, -0.6170],\n",
            "         [ 0.1772, -0.6665],\n",
            "         [ 0.4053, -0.5249]],\n",
            "\n",
            "        [[ 0.8008,  1.6806],\n",
            "         [ 0.5783,  0.4970],\n",
            "         [ 0.2211,  0.4118],\n",
            "         [-0.1119,  0.3318],\n",
            "         [-0.1398,  0.4374],\n",
            "         [-0.1682,  0.2985],\n",
            "         [-0.0294,  0.1671],\n",
            "         [-0.0997,  0.1383]],\n",
            "\n",
            "        [[ 0.3057, -0.7746],\n",
            "         [ 0.1703, -0.2267],\n",
            "         [ 0.6381, -0.4330],\n",
            "         [ 0.8066, -0.1529],\n",
            "         [ 0.3984, -0.2199],\n",
            "         [ 0.0956, -0.0339],\n",
            "         [ 0.0891,  0.2948],\n",
            "         [ 0.2253,  0.2036]],\n",
            "\n",
            "        [[-0.8140, -0.7360],\n",
            "         [-0.8256, -0.8292],\n",
            "         [ 0.0534, -0.4993],\n",
            "         [ 0.1319, -0.3306],\n",
            "         [-0.1314,  0.0122],\n",
            "         [-0.3099,  0.1282],\n",
            "         [-0.4193,  0.1864],\n",
            "         [-0.2200,  0.2332]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvmIqfrES_8y",
        "outputId": "58a0fa81-a51b-45ca-df39-043bcbd5268a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhoZYPgeXCDm",
        "outputId": "4d63e44a-fef8-46f4-db31-79749b97171f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFQ2WXkHlv0X",
        "outputId": "6b824573-1320-4e8b-de15-53873419531f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('Oromay-BaaluGirma.utf8.txt.1', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-WhHJcPl5lN",
        "outputId": "0cb5bf1a-22ef-4372-af42-2b5217774c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.238367 M parameters\n",
            "step 0: train loss 5.8103, val loss 5.8181\n",
            "step 100: train loss 3.5926, val loss 3.4975\n",
            "step 200: train loss 3.3179, val loss 3.2149\n",
            "step 300: train loss 3.2057, val loss 3.0994\n",
            "step 400: train loss 3.1096, val loss 3.0484\n",
            "step 500: train loss 3.0598, val loss 2.9716\n",
            "step 600: train loss 2.9848, val loss 2.9102\n",
            "step 700: train loss 2.9245, val loss 2.8707\n",
            "step 800: train loss 2.8947, val loss 2.8053\n",
            "step 900: train loss 2.8383, val loss 2.7742\n",
            "step 1000: train loss 2.8002, val loss 2.7369\n",
            "step 1100: train loss 2.7588, val loss 2.6909\n",
            "step 1200: train loss 2.7393, val loss 2.6706\n",
            "step 1300: train loss 2.7026, val loss 2.6220\n",
            "step 1400: train loss 2.6473, val loss 2.6029\n",
            "step 1500: train loss 2.6417, val loss 2.5799\n",
            "step 1600: train loss 2.6096, val loss 2.5487\n",
            "step 1700: train loss 2.5975, val loss 2.5547\n",
            "step 1800: train loss 2.5662, val loss 2.5102\n",
            "step 1900: train loss 2.5596, val loss 2.5189\n",
            "step 2000: train loss 2.5288, val loss 2.5090\n",
            "step 2100: train loss 2.5095, val loss 2.4756\n",
            "step 2200: train loss 2.5006, val loss 2.4726\n",
            "step 2300: train loss 2.4795, val loss 2.4614\n",
            "step 2400: train loss 2.4648, val loss 2.4355\n",
            "step 2500: train loss 2.4390, val loss 2.4447\n",
            "step 2600: train loss 2.4352, val loss 2.4164\n",
            "step 2700: train loss 2.4435, val loss 2.3985\n",
            "step 2800: train loss 2.4090, val loss 2.3948\n",
            "step 2900: train loss 2.4118, val loss 2.3985\n",
            "step 3000: train loss 2.4082, val loss 2.3746\n",
            "step 3100: train loss 2.3781, val loss 2.3790\n",
            "step 3200: train loss 2.3795, val loss 2.3613\n",
            "step 3300: train loss 2.3621, val loss 2.3681\n",
            "step 3400: train loss 2.3498, val loss 2.3447\n",
            "step 3500: train loss 2.3328, val loss 2.3288\n",
            "step 3600: train loss 2.3453, val loss 2.3470\n",
            "step 3700: train loss 2.3258, val loss 2.3298\n",
            "step 3800: train loss 2.3512, val loss 2.3339\n",
            "step 3900: train loss 2.3079, val loss 2.3193\n",
            "step 4000: train loss 2.3167, val loss 2.3464\n",
            "step 4100: train loss 2.2989, val loss 2.3072\n",
            "step 4200: train loss 2.2993, val loss 2.3070\n",
            "step 4300: train loss 2.3002, val loss 2.2966\n",
            "step 4400: train loss 2.2858, val loss 2.2881\n",
            "step 4500: train loss 2.2759, val loss 2.2859\n",
            "step 4600: train loss 2.2702, val loss 2.3050\n",
            "step 4700: train loss 2.2764, val loss 2.2881\n",
            "step 4800: train loss 2.2627, val loss 2.2761\n",
            "step 4900: train loss 2.2620, val loss 2.2935\n",
            "step 4999: train loss 2.2461, val loss 2.2650\n",
            "\t á‰°áˆ¨áŒˆá‹ á‰°á‰€áˆ­á‰·áˆá‰µ á¢\n",
            "áˆ˜áˆˆá‹¨á‰µáˆ á‹­á‰³á‹«áˆ á‰µáŒáˆ­ á‹•áˆˆá‰± á‹­áˆˆá‰³áˆ‰ á¢ á‹­áˆ… áˆ´á‰µ Â« áˆáˆµá‰µá‰€á‹µáŠ³á‹­ á‰†áˆµá‹á‹ á‹­á‰°á‹˜á‹ Â» áŠ áˆˆáŠ á¢\n",
            "     Â« áŠ¥á‹áŠá‰±áŠ• á‹¨áˆ˜á‰¸áˆ á‰°áŠáˆ³ Â« áˆ‚á‹›á‰µáŠ“á‹ áŠá‰ áˆ­ ...Â»\n",
            "     Â« áŠá‹«áˆœá‰³ áˆ°á‹á‰½ á‹¨áŠ¨á‰°á‹°áˆ¨áˆ± áˆ¥á‰£áŠ•á‰ áŒ¥áŠ• á‹³áŠ•á‹µ áŠ­áá‰ƒáˆˆáˆ á‹ˆáŠ•áŒ†á‰½áŠ• á‹¨á‰€á‹­ áˆ˜áŠ•áŒˆá‹±á‰ á‰µ áˆ‹á‹­ áŒˆá‹±áˆ™á‹ ? áŠ¨á‹šá‹« áˆá‰µáŠá‰ áˆ©áŠ• á‰£áˆ¬áˆ½ á‰³áŠ…\n",
            "          áˆˆáŒáŠ• áˆ˜áˆáˆ°áˆˆáŠ á¢ áŠ áŠ•áˆµá‰¶ áˆ²á‰£áˆ‰ áŠáŒˆáˆ­ á‰ á‹­á‰°áŠáŠ¨áˆ¨áˆ±áˆ á‹¨áˆˆáˆ á¢ áˆ†áŠ– á‹¨áˆºá‹áˆ˜ áˆšá‹«áˆµá‹°áˆ­áˆµá‰¡á‰µ á‰ áˆ›áˆˆá‰µ á‹­á‰»áˆ á‹ˆáˆ­á‹šá‹« áŒ®áŠ¸á‰¥ á¢ á‹¨áŒáŠ• ááŒ¡áˆ­ á‹¨á‰½áŒáˆ© á‹­á‰³á‹«áˆ áˆáŠ•áˆ á‰ áˆ­á‰»á‹¬áŠ• 5 á‹°áŠ•á‹µ á¢\n",
            "     Â« áŒ“á‹µ á‹¨áˆá‰»áˆˆá‹áŠ• áŠ áŠ•á‰ºá‹­ á‰£áˆˆáˆ¥áˆáŒ£áŠ• á‰ áˆáˆ›á‹ á‹µáˆ«á‰µ áˆµá‹áˆ áŠá‹ á‹¨á‰ áŒ£áˆ á¢    \n",
            "     Â« áŠ áŠ• áˆ˜áˆáˆªá‹«á‹ áŠá‰ áˆ­ ! áŠ áˆáŠ©á‰µ áŒ€áˆ˜áˆ« áŠá‰ áˆ­ á¢ áŠ áŠ•á‹´ áˆ˜áˆ áˆ¨á‰°áŠ›áŠ á‰µáŠ• -- áŠ¨áŠ•áŒáˆ© áŠ¨á‹“á‹­á•áˆ‹á Â« áŒ‹áˆ¹ á¢ Â»\n",
            "     Â« áŠ­áˆ­áŠ”áˆ á‰³áŒ¥áŠ¨á‹ á‰ áŒ¥áˆ‰ á‹°áˆµ á‰¥áˆ­ áˆµáˆáŠ­ - áŒ¦áˆ«á‰½ '  ááˆ® á‰ áˆ¥áˆáŒ£áŠ• á‰µáŠ•áŠ›á‹ áˆµáˆáŠá‰µ á‹¨áˆšáˆˆáŠ áŠ á‰µáŠ¨áˆ­áŠ©áŠ• á¢ áŠ¨áá‰°áŠ› á‹ˆáˆ‹á‰¥ á‹µáˆ­áŒ…á‰¶á‰½ á‹¨áˆšá‹«á‹°áˆ­áŒ‰á‰µ áŠ áŠ•á‹µ á‰ áŠ áˆ¥áˆ˜áˆ« áŠ¥á‹¨á‰°á‰£áˆˆá‰€ áŒ áŠ­áˆ½ áŠá‰ áˆ­ áŠ á‰ á‰£áŠ á¢ á‰ áŠ«áˆ­á‰¶áŠ‘áŠ• á¤ áŒŠá‹œ áˆˆáŠá‹«áˆœá‰³á‹áŠ• áŠ¥áŠ•á‹° áˆ•á‹áŠ®áˆ­áŠ• áŠ¨áŠá‰ áˆ¨ áˆ‹á‹­ áŒ á‹¨áˆµá‰¦áŠ á‰ áˆ á‹áŠ•á‰³á‰¸á‹ á‰ á‹¨áŒ¦á‹´á‹«áˆœá‹³ áˆ áˆ‹áˆ áŠ áˆá‹ˆá‰´ á‹¨áˆ˜áˆáŠ¨á‰µ áŒ‰áˆ á‰¡áŠ“ áŠ¨áˆ áˆ‹áˆ Â« áŠ áŒ áˆˆáŠ³á‰µ á¢ áŠ¨á‰€á‹­ á‹µáˆáŒ½ á‹ˆá‹° áŠ á‰¥áˆ¨áŠ• á‹¨á‹˜á‰¥ á‹“á‹­á‰½áŠ• áˆ™ á‰ áˆ«áˆ‚á‹°áŠ áˆµáŠ•áŒˆáˆ­ áˆšáŠ–áˆ­á‰ á‰µáŠ• áŠ á‹­áˆµá‰ áŠáˆ  á¢ áŠ¥áŠá‹²áˆ… á‹«áˆˆá‰ á‰ƒáˆáŠá‹³á‰¸á‹‹áˆ á¢ á‰¤á‰µ á‹“áˆ­á‰¶ á‰ á‹¨áŒ á‹¨ áŒ‹áˆ­ Â« á‹°áˆá‹µáˆ áŠ¥áŠ•á‹°áŒˆá‰£á‹­ á‰€áŠ• áˆ˜áŒ£áˆ ?\n",
            "       Â« áŠ¢á‹›á‰´ áŠ¥á‰ƒáˆµáˆ áŠ áŒ¤áŠ’áŠ“ áˆŠá‰†áŒ¥ á¤ áŠ¥áŒ… áˆ•á‹­á‹ˆá‰± á¤ á‹µáˆ¨áˆµáŠ­ áŠ¥á‹«áŠ•á‹³á‹­ á‰£áŠ•á‰º áŠ á‹²áˆµ ? á‹¨á‰°á‹˜á‹‹áŒŠáŠ•á‰µ á•áˆ®á“áŒ‹áŠ•á‹±áˆ áˆá‰¶ á‹ˆáŒ¥áˆ®áŠ á‰¸á‹áŠ• áˆá‰µáˆáŠ•á‹µ áŠá‹ á¢ á‹¨áˆ¥á‹•áˆ‹á‹­ á‹¨á‹ˆáŠ•á‰ á‹´á‹á‰½áŠ• áŠ á‹²áŠ«á‹Š áˆ á‹šáŠ¨á‰¥ áŠ á‹µáˆ­ áˆ¥áˆ« á¢ á‰…áˆ­á‰¶ áŠ á‰µá‹ˆá‹µáˆ á¢ áŒ€á‰¥áˆ ááŒ¹áˆ Â»\n",
            "     Â« áŠ¨áŠ” áŠ­ááˆˆ á‹“áˆá‰³á‰½áˆ áŠá‹ á¢ áŠ¥á‹šá‹« á‰¤á‰¶á‰½ á‹¨áˆ«áˆµáŠ•áŠ• áˆˆá‹˜á‹´á‹á‰½ á‰µáˆ¨áˆ½áŠ• á¢ áŠ¥á‹¨á‰°á‰»áˆá‹ áˆ˜áˆáˆ¼ á‹«áˆˆá‹ á‹°áˆ­áŒˆáŠ›áˆ á‰ áˆ«áˆµ á‰ á‹ˆáŠ•áŒ‚áŠ­ áˆáŠ•áˆ áŠ áŠ•á‹³áŠ•áˆáŠ• áˆˆáˆ˜áŒ€á‰ á‹ á‰ áŒá‹²áŠ¸áŠ áŠ¨á‹°á‰‚á‰ƒ áˆ³á‰¤ áŒáŠ•áˆ˜áˆµá‹‹ áˆá‰¥ áˆˆáˆ˜áŠ•áŒˆá‹±áŠ•áˆ á‰°á‰ áˆ‹áˆˆ á¢ áˆ¥á‹•áˆ‹á‹­ á‰ áŠ¥áŒ®áŠ› áŠ áˆ£áŠ¨áŠ•á‰²áŠ­ á‰¸áˆˆáˆ›á‹­ á‰³áŠ®áŠ¨á‰µ áˆ˜áŠ”áˆ áˆ†áŠ•áŠ® áŠá‰ áˆ© -\n",
            "áŒ¥áŒ§á‰± - á‰£áˆ•áˆ«á‰¥ á‹µáˆ­ áˆ˜áˆáˆµáˆ áˆáŠ”á‰³ á‹­áŒˆá‰£áŠ™á‹¨á‰¥áŠ›áˆ á¢ á‹«á‹™ áˆáŒ£áŠ• á‹¨á‰µ á‰°á‹ˆáŒ£áŒ¥áˆ® áˆ˜áŠ•áŒˆá‹¶á‰½ á‰ áŠ¥áˆ¥áŠ­ á‰ á‰£áˆ•áˆ­ áˆ•á‹á‰¡áŠ• á‹ˆáŒ£á‹á‰½ á‹“áˆˆáˆ á‰°áˆ³áˆµá‰³ áŠ áˆˆá‰ á‰µáŠ• á¢ áŠ¨á‰°áˆ› á‹°á‹áˆ áŠ á‰¥áˆá‰±áŠ• áŒ¸áˆ˜á‰µ á‹­áˆ… áŠ áŒ‰áˆ áŠ á‹›á‹¦á‰½ áŠ áˆˆáŠ á¢  áŠ¥á‹áŠ• áˆ¨á‹ˆáŒˆáŠ á‰ áˆ¦áˆµá‰´ áˆáŠ”á‰³á‹áˆ  á‰°áˆá‰¸á‹ á‹­áˆ‹áˆ á¢ áá‰…áŠ“ áŒ¦áˆ­áŠ áŠ áŒ á‰ áŠ©áˆ­áŠ©á‰µ á‰¥á‰» áŠá‰ áˆ­ á¢áˆˆáˆáŠ• áˆˆáˆ›áŠ• áŠ á‹²áˆµ áŒ áŠ•áˆµ áˆˆáˆáŠ• á‹“á‹­áŠá‰µ áŠá‰ áˆ­ áŠ¨áŠáˆ± á¢ áŠ¥áŠ“ áŠ¥áˆº á‰ áˆ›áˆˆá‰µ 102 áˆˆáŒ¥áˆ© áˆá‰¥ á‰°áŒ£áŒ á‹ á¢ áŠ áŠ•á‹³áŠ•á‹µ áˆ³á‹­ á¢ á‰ áŠ¥áˆ± á‰ á‰£á‹•á‹µá‹áˆ áŠáŒˆáˆ­ áŠ áˆá‰°á‰ áˆáŠ©áˆ á¢ áŠ¨áŒáˆ¥áŠ› á‹«á‰ áŒ¥áˆ«áˆ‰ á‰ á‹šáˆ… á‹°áŒ á‰¥áˆ­ áŠáŠ á¢ áˆˆáˆá‰±á‰µ áŠ¥áŠ•á‹° áŠ¥áŠ•á‹³á‹­ áŒŠá‹œ á‹¨áˆšáˆ˜áŒ¡ áˆá‰¥ á‰ áŠ¥á‹áŠá‰µ á‰£áˆ³á‰¥  á‹•á‹³áŒ‹ áˆáŠ•á‰³ á‹ˆáŠ•á‰ á‹´ á‹¨áˆ°á‹ áŠ¥áŒá‰¾á‰½áŠ•áŠ• á‰ áŠ á‹­á‰€áˆ á‹¨á‰µ áŠ áˆˆ áˆ°áˆˆá‹‹ á‰ á‹áŠ• áˆ¥áˆ­ á‰ áˆ˜áˆ”á‹µ áˆ¥áˆ›á‰½ áŠ¨áˆ˜áŒ¨áˆ¨áˆ» á‰£áŒáˆ¨áˆ»á‰µ á‹¨áˆ°á‰¥áŠ áˆ¨áˆ£áˆ•áˆ á‹¨á‰°áˆ‰ áˆ•áˆŠá‹áŒ¥ áŒ¥ááˆ«á‰µ áˆ¨áŒ¢á‰¿ á‰µáˆ á‰¸á‹‹áˆ á‰µáŠ•á‰ áŒ¥á‰°á‹‹áˆˆá‰½ á¢ á‰ áˆ«áˆ±áŠ• áŒ¥áˆ©áŠ á‰µ á‹¨áˆˆáˆ á¢ á‹á‹‹áŠ•á‰µ áˆˆáˆ˜áˆáˆ¨áˆ³ á‰ á‰£á‹­áŠ•á‰´ á‹œáŒ áŠá‰½ á‹áˆá‰³ '  á‹áŒƒá‹ áˆµáŠ•áˆ³áŠ­áˆ«á‰¸á‹ áŠ áŠ•á‹µ á‹°áˆµ áˆ³á‹­ á‹ˆá‹° áˆáˆ­ áŒ¸áˆáŒ€ á‹­áŒ‹ á‹«á‹¨á‰°á‹˜áŒ‹ á‹­áŠ­áˆµáŠª áŒ áŠ•á‹¶á‰½ á‹¨á‹ˆá‹­áŠ“áˆ á‰°áŒ á‹¨á‹áŠ“ áŠ á‹á‰¥áˆ«á‰½áˆ á¢ áˆ°á‹ á‹ˆá‹­áˆ á‰¸áŒ áŠ¨á‹šá‹«áŠ• áˆˆáˆ˜áŠ¨á‰¦á‰½á‹ áŠ¥áŠ•á‹° áˆáŠ­áŠ•á‹²áˆ… áŒ á‰…áŒ áˆ­ á‹¨áˆšá‰¸á‹áŠ• á‰ áŠ­áˆ˜áŠáˆ­ áŠ á‹­áˆ¨áˆ¸á‹ á¢ á‰ á‹šá‹« á‹¨áŠáˆ áŠ¨áˆ˜áŒ¨áˆ¨áˆ» áŠ áˆ‹á‹á‰…áˆ á¢ áˆáˆá‰…áŒ á‹ á¤ áˆ áˆ­á‰³á‰½áŠ• á‰ áˆ˜á‰°áˆ¨áˆ»á‹« áˆ°á‰µáŠ© á‹¨áˆšá‹«áˆµá‰¥ á‹¨áˆáŠ•á‰€áŠ›áˆ áŠ¥áŠ•á‹²áˆ”á‹±áˆ® á‰°áˆ˜áˆˆáŠ¨á‰µáŠ© á¢ áˆµáˆáŠ­ áˆ•á‹­á‹ˆáˆ˜á‹áŠ• á‹­á‰³á‹«áˆ á¢ á‰ á‰°áˆˆáŠá‰µ á¢ áŠ¥áŠ•á‹²á‹«á‹ áŒ¥á‰…áˆ á‹°áŒáˆ á¢ áˆ«áˆ±áŠ•  á‹ˆá‹° áŠ¨áˆá‰€áŒ¥ áŠ®áŠ¨á‰¥ áˆ˜áŠ•áŒˆá‹µáˆ›á‰µ  áŠƒáˆ‹áŠ á‹¨á‰³áˆªáŠ© áŠ áŒˆá‰£á‹ áˆˆáˆ˜á‰»áˆá¢\n",
            "áŒ½áˆ©áŠ• áˆ¥áŠ á‹•á‰ƒáŠ áˆ áˆ«á‹ŠáŠ¨áˆ á‹«áˆˆááˆ á‰£áˆá‰¹ áŠ¨áŒ€á‰¥á‹°áŠ› á‹¨á‰°áˆ˜áˆˆáˆµáŠ© á‰°á‹°á‹°áŠ¨á‰°á‹ áŒˆáŠ˜á‰µ á‰£á‹µáˆ­ áˆ²áŒ áŒ©á‰ á‰… áˆˆáŒ¥áˆá‰¥ á‹¨áˆšáˆˆáŠá‰µ áˆ˜áˆµáˆˆ áŠƒá‹­áˆˆáˆ›á‹‹áˆ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHLzQF3JwEFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}